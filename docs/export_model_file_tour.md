# Exported MMS Model File Tour

In the quick start export example on the main [README](../README.md), we provide a zip file of all of the artifacts you need to run your first export. However, when you start using MMS and downloading and sharing models you will be using `.model` files, so here we'll start with one of those.

**Download and extract a model file**,
*or choose a model file from the [model zoo](model_zoo.md)*:

```
curl -O https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1.model
```

**Important**: A `.model` file is a zip file under the hood, so if you have trouble extracting it, change the extension to `.zip` first and then extract it. It might be worth assigning your favorite unzip program to the `.model` filetype. You also might be able to just use unzip from the terminal:

```bash
unzip squeezenet_v1.model
```

**Once the model archive has been extracted you can review the following files:**

* **Manifest** (json file) - a description of the files in the archive that is generated by `mxnet-model-export`.
  * Example: [MANIFEST.json](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/manifest.json) - it describes each of the artifacts, the deep learning engine to use, and metadata about versions, and more.


* **Model Definition** (json file) - contains the description of the layers and overall structure of the neural network.
  * Example: [squeezenet_v1.1-symbol.json](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1-symbol.json) - the name, or prefix, here is "squeezenet_v1.1".


* **Model Parameters and Weights** (binary params file) - contains the parameters and the weights.
  * Example: [squeezenet_v1.1-0000.params](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/squeezenet_v1.1-0000.params) - again, the prefix is "squeezenet_v1.1".


* **Model Signature** (json file) - defines the inputs and outputs that MMS is expecting to hand-off to the API.
  * Example: [signature.json](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/signature.json) - in this case for squeezenet_v1, it expects images of 224x224 pixels and will output a tensor of 1,000 probabilities.


* **Custom Service** (py file) - customizes the inference request handling for both pre-processing and post-processing.
  * Example: [custom-service.py](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/mxnet_vision_service.py) - in this case, it is a copy of the [mxnet_vision_service.py](https://github.com/awslabs/mxnet-model-server/blob/master/mms/model_service/mxnet_vision_service.py) which does standard image pre-processing to match the input required and limits the output results to the top 5 instead of the full 1,000.

* **assets** (text files) - auxiliary files that support model inference such as vocabularies, labels, etc. Will vary depending on the model.
  * Example:  [synset.txt](https://s3.amazonaws.com/model-server/models/squeezenet_v1.1/synset.txt) - an *optional* list of labels (one per line) specific to a image recognition model, in this case based on the ImageNet dataset.
  * Example:  [vocab_dict.txt](https://s3.amazonaws.com/model-server/models/lstm_ptb/vocab_dict.txt) - an *optional* list of word/index pairs specific to an LSTM model, in this case based on the PenTreeBank dataset.
